# client_http.py


import json
import requests


# LangChain
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

import os
from dotenv import load_dotenv
load_dotenv()

# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹
model = ChatOpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url=os.getenv("BASE_URL"),
    model=os.getenv("MODEL"),
)

server_url = "http://127.0.0.1:8000"

def fetch_tools():
    resp = requests.get(f"{server_url}/tools")
    all_items = resp.json()
    return all_items, all_items  # èµ„æºä¸å·¥å…·ç»Ÿä¸€åˆ—è¡¨

def ask_llm_to_select_tool_and_resource(user_question, tools, resources):
    tools_desc = "\n".join([
        f"- {t['name']}: {t.get('desc', t.get('description', ''))} (method: {t.get('method', 'GET')}, url: {t.get('url', '')})"
        for t in tools
    ])

    resources_desc = "\n".join([
        f"- {r['name']}: {r.get('desc', r.get('description', ''))} (url: {r.get('url', '')})"
        for r in resources
    ])


    system_prompt = f'''
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œç”¨æˆ·æå‡ºé—®é¢˜åï¼Œä½ é¦–å…ˆè€ƒè™‘è¦ä¸è¦ä½¿ç”¨å·¥å…·åˆ—è¡¨ä¸­å’Œèµ„æºåˆ—è¡¨ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œè°ƒç”¨ï¼Œ
å¹¶é€‰æ‹©æ˜¯å¦éœ€è¦é…åˆèµ„æºã€‚ä½ çš„è¾“å‡ºåº”ä¸ºä¸€ä¸ª JSONï¼Œå¯¹åº”å­—æ®µå¦‚ä¸‹ï¼š
tool: {{ name: å·¥å…·å, url: å·¥å…·è·¯å¾„, method: è¯·æ±‚æ–¹å¼, params: {{å‚æ•°}} }},
resource: {{ name: èµ„æºå, desc: ç®€è¦è¯´æ˜ }}ã€‚
è¯·ä¸è¦éšæ„æ”¹å˜å·¥å…·çš„urlã€‚
å¦‚æœä¸éœ€è¦èµ„æºï¼Œresource è®¾ç½®ä¸º nullã€‚
ä»…ä½¿ç”¨ä»¥ä¸‹å·¥å…·åˆ—è¡¨ï¼š
{tools_desc}

ä»¥ä¸‹æ˜¯å¯ç”¨èµ„æºï¼š
{resources_desc}
'''


    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_question)
    ]
    print("ğŸ“¤ æç¤ºè¯å·²å‘é€åˆ° LLMï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...")
    response = model.invoke(messages)
    print("ğŸ“¥ LLM è¿”å›å†…å®¹ï¼š", response.content)

    try:
        return json.loads(response.content)
    except json.JSONDecodeError:
        print("âŒ LLM è¿”å›æ ¼å¼é”™è¯¯")
        raise

def call_tool(tool):
    url = f"{server_url}{tool.get('url')}"
    method = tool.get("method", "GET").upper()
    params = tool.get("params", {})

    print(f"ğŸ‘‰ è¯·æ±‚ URL: {url}")
    print(f"ğŸ‘‰ è¯·æ±‚æ–¹æ³•: {method}")
    print(f"ğŸ‘‰ è¯·æ±‚å‚æ•°: {params}")

    if method == "GET":
        response = requests.get(url, params=params)
    else:
        raise NotImplementedError(f"ä»…æ”¯æŒ GET æ–¹æ³•ï¼Œå½“å‰ä¸º {method}")

    try:
        return response.json()["result"]
    except Exception as e:
        print("âŒ å·¥å…·è°ƒç”¨å¼‚å¸¸ï¼Œè¿”å›å†…å®¹ï¼š", response.text)
        raise e

def summarize_answer(question, raw_result):
    messages = [
        SystemMessage(content="è¯·æ ¹æ®è°ƒç”¨è¿”å›çš„åŸå§‹ä¿¡æ¯ï¼Œæ€»ç»“å‡ºä¸€ä¸ªç®€æ˜æ¸…æ™°çš„ä¸­æ–‡ç­”æ¡ˆã€‚"),
        HumanMessage(content=f"é—®é¢˜ï¼š{question}\nè°ƒç”¨ç»“æœï¼š{raw_result}")
    ]
    response = model.invoke(messages)
    return response.content

def main():
    user_question = input("è¯·è¾“å…¥é—®é¢˜ï¼š")
    # user_question = "1+2=?"
    print("Step 1: è·å–å·¥å…·å’Œèµ„æº...")
    tools, resources = fetch_tools()

    max_retries = 3
    for attempt in range(max_retries):
        print(f"\nStep 2: ä½¿ç”¨ LLM é€‰æ‹©å·¥å…·å’Œèµ„æº...ï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰")
        try:
            llm_selection_json = ask_llm_to_select_tool_and_resource(user_question, tools, resources)
            tool = llm_selection_json["tool"]
            resource = llm_selection_json.get("resource")

            print(f"Step 3: ä½¿ç”¨å·¥å…· {tool['name']} è°ƒç”¨æ¥å£ {tool['url']} ...")
            result = call_tool(tool)

            print("Step 4: ä½¿ç”¨ LLM æ€»ç»“ç­”æ¡ˆ...")
            summary = summarize_answer(user_question, result)
            print("\nâœ… æœ€ç»ˆç­”æ¡ˆï¼š", summary)
            break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯

        except Exception as e:
            print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡æ‰§è¡Œå¤±è´¥ï¼š{e}")
            if attempt < max_retries - 1:
                print("ğŸ§  å‘ LLM åé¦ˆé”™è¯¯ï¼Œå°è¯•ç”Ÿæˆæ–°çš„è°ƒç”¨æ–¹æ¡ˆ...")
                messages = [
                    SystemMessage(content="ä½ åˆšæ‰è¾“å‡ºçš„ JSON æ— æ³•æˆåŠŸè°ƒç”¨æ¥å£ï¼Œè¿”å›äº†é”™è¯¯ä¿¡æ¯ã€‚è¯·åŸºäºä»¥ä¸‹é”™è¯¯ä¿¡æ¯ï¼Œä¿®å¤è¾“å‡ºã€‚"),
                    HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{user_question}\né”™è¯¯ä¿¡æ¯ï¼š{str(e)}")
                ]
                response = model.invoke(messages)
                print("ğŸ“¥ LLM ä¿®å¤è¾“å‡ºå†…å®¹ï¼š", response.content)
                try:
                    llm_selection_json = json.loads(response.content)
                except:
                    print("âŒ ä¿®å¤è¾“å‡ºä»æ— æ³•è§£æä¸º JSONï¼Œå°†é‡æ–°å°è¯•...")
            else:
                print("âŒ å¤šæ¬¡å°è¯•å‡å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")

if __name__ == "__main__":
    main()
